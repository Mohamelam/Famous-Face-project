{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FaceNet (InceptionResnetV1) + ArcFace Training\n",
    "\n",
    "This notebook trains a FaceNet model using the `facenet-pytorch` library with an ArcFace head on the 105_classes_pins_dataset.\n",
    "\n",
    "## Key Features:\n",
    "- **Backbone**: InceptionResnetV1 (pretrained on VGGFace2)\n",
    "- **Loss**: ArcFace (Additive Angular Margin Loss)\n",
    "- **Optimizer**: SGD with OneCycleLR scheduler\n",
    "- **Image Size**: 160x160 (Native resolution for FaceNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T13:20:56.898652Z",
     "iopub.status.busy": "2025-11-26T13:20:56.897964Z",
     "iopub.status.idle": "2025-11-26T13:23:40.180088Z",
     "shell.execute_reply": "2025-11-26T13:23:40.179172Z",
     "shell.execute_reply.started": "2025-11-26T13:20:56.898624Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.17.2)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.24.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
      "Collecting torch==2.9.1 (from torchvision)\n",
      "  Downloading torch-2.9.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch==2.9.1->torchvision)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision) (2025.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.9.1->torchvision)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.9.1->torchvision)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.9.1->torchvision)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch==2.9.1->torchvision)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.9.1->torchvision)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.9.1->torchvision)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.9.1->torchvision)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.9.1->torchvision)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.9.1->torchvision)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch==2.9.1->torchvision)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch==2.9.1->torchvision)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch==2.9.1->torchvision)\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.9.1->torchvision)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.9.1->torchvision)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.9.1->torchvision)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch==2.9.1->torchvision)\n",
      "  Downloading triton-3.5.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.9.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.9.1->torchvision) (3.0.3)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\n",
      "Downloading torchvision-0.24.1-cp311-cp311-manylinux_2_28_x86_64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.9.1-cp311-cp311-manylinux_2_28_x86_64.whl (899.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m899.8/899.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m770.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.5.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.4/170.4 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
      "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
      "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
      "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
      "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
      "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.2\n",
      "    Uninstalling torch-2.2.2:\n",
      "      Successfully uninstalled torch-2.2.2\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.17.2\n",
      "    Uninstalling torchvision-0.17.2:\n",
      "      Successfully uninstalled torchvision-0.17.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "facenet-pytorch 2.6.0 requires torch<2.3.0,>=2.2.0, but you have torch 2.9.1 which is incompatible.\n",
      "facenet-pytorch 2.6.0 requires torchvision<0.18.0,>=0.17.0, but you have torchvision 0.24.1 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.9.1 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 sympy-1.14.0 torch-2.9.1 torchvision-0.24.1 triton-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torchvision\n",
    "!pip install facenet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T13:23:59.680085Z",
     "iopub.status.busy": "2025-11-26T13:23:59.679793Z",
     "iopub.status.idle": "2025-11-26T13:23:59.711247Z",
     "shell.execute_reply": "2025-11-26T13:23:59.710272Z",
     "shell.execute_reply.started": "2025-11-26T13:23:59.680056Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venvDLproject (3.10.0) (Python 3.10.0)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"d:/3rd first term/Deep Learning/front celeb/.venvDLproject/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "\n",
    "# Configuration\n",
    "DATASET_PATH = \"/kaggle/input/pins-celebfaces/105_classes_pins_dataset\"\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 160  # FaceNet native size\n",
    "EMBEDDING_SIZE = 512\n",
    "NUM_CLASSES = 105\n",
    "EPOCHS = 25\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-26T13:20:00.516977Z",
     "iopub.status.idle": "2025-11-26T13:20:00.517308Z",
     "shell.execute_reply": "2025-11-26T13:20:00.517152Z",
     "shell.execute_reply.started": "2025-11-26T13:20:00.517138Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "# Transforms - Using 160x160 for FaceNet\n",
    "traintransform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # FaceNet expects [-1, 1] usually, or standard normalization. \n",
    "    # Note: facenet-pytorch's fixed_image_standardization is approx mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]\n",
    "])\n",
    "\n",
    "valtransform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load Dataset\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    full_dataset = datasets.ImageFolder(DATASET_PATH)\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_dataset = TransformSubset(train_dataset, traintransform)\n",
    "    val_dataset = TransformSubset(val_dataset, valtransform)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "        num_workers=4, pin_memory=True, persistent_workers=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "        num_workers=4, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Classes: {len(full_dataset.classes)}\")\n",
    "    print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)}\")\n",
    "else:\n",
    "    print(\"WARNING: Dataset path not found. Please check the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture\n",
    "\n",
    "We use `InceptionResnetV1` as the backbone and attach an `ArcFaceMarginProduct` head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-26T13:20:00.518229Z",
     "iopub.status.idle": "2025-11-26T13:20:00.518574Z",
     "shell.execute_reply": "2025-11-26T13:20:00.518410Z",
     "shell.execute_reply.started": "2025-11-26T13:20:00.518392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ArcFaceMarginProduct(nn.Module):\n",
    "    \"\"\"ArcFace margin product\"\"\"\n",
    "    def __init__(self, embedding_size, n_classes, s=30.0, m=0.5, easy_margin=False):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(n_classes, embedding_size))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "        self.easy_margin = easy_margin\n",
    "\n",
    "    def forward(self, embeddings, labels=None):\n",
    "        W = F.normalize(self.weight, dim=1)\n",
    "        cos_theta = torch.mm(embeddings, W.t()).clamp(-1.0, 1.0)\n",
    "\n",
    "        if labels is None:\n",
    "            return cos_theta * self.s\n",
    "\n",
    "        cos_y = cos_theta.gather(1, labels.view(-1, 1)).view(-1)\n",
    "        sin_theta = torch.sqrt(1.0 - cos_y * cos_y).clamp(0.0, 1.0)\n",
    "        phi = cos_y * self.cos_m - sin_theta * self.sin_m\n",
    "\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cos_y > 0, phi, cos_y)\n",
    "        else:\n",
    "            phi = torch.where(cos_y > self.th, phi, cos_y - self.mm)\n",
    "\n",
    "        logits = cos_theta.clone()\n",
    "        logits.scatter_(1, labels.view(-1, 1), phi.view(-1, 1))\n",
    "        logits = logits * self.s\n",
    "        return logits\n",
    "\n",
    "class FaceNetArcFace(nn.Module):\n",
    "    def __init__(self, n_classes, embedding_size=512, pretrained='vggface2', s=30.0, m=0.5):\n",
    "        super().__init__()\n",
    "        # Load pretrained FaceNet (InceptionResnetV1)\n",
    "        self.backbone = InceptionResnetV1(pretrained=pretrained, classify=False)\n",
    "        \n",
    "        # ArcFace Head\n",
    "        self.margin_product = ArcFaceMarginProduct(\n",
    "            embedding_size=embedding_size, \n",
    "            n_classes=n_classes, \n",
    "            s=s, \n",
    "            m=m\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        # Get embeddings from backbone\n",
    "        embeddings = self.backbone(x)\n",
    "        \n",
    "        # InceptionResnetV1 output is already normalized if classify=False? \n",
    "        # Actually, facenet-pytorch implementation does NOT normalize by default if classify=False.\n",
    "        # It returns x which is the output of the last BN layer.\n",
    "        # So we should normalize it for ArcFace.\n",
    "        embeddings = F.normalize(embeddings, dim=1)\n",
    "        \n",
    "        logits = self.margin_product(embeddings, labels)\n",
    "        return logits, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-26T13:20:00.521003Z",
     "iopub.status.idle": "2025-11-26T13:20:00.521372Z",
     "shell.execute_reply": "2025-11-26T13:20:00.521199Z",
     "shell.execute_reply.started": "2025-11-26T13:20:00.521184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, criterion, scaler, scheduler, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch} [TRAIN]\")\n",
    "\n",
    "    for imgs, labels in loop:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            logits, _ = model(imgs, labels)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        loop.set_postfix(loss=loss.item(), acc=correct/total)\n",
    "\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(val_loader, desc=f\"Epoch {epoch} [VAL]\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loop:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            with autocast():\n",
    "                logits, _ = model(imgs, labels)\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            preds = logits.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            loop.set_postfix(loss=loss.item(), acc=correct/total)\n",
    "\n",
    "    return running_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-26T13:20:00.522285Z",
     "iopub.status.idle": "2025-11-26T13:20:00.522643Z",
     "shell.execute_reply": "2025-11-26T13:20:00.522456Z",
     "shell.execute_reply.started": "2025-11-26T13:20:00.522439Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize Model\n",
    "model = FaceNetArcFace(\n",
    "    n_classes=NUM_CLASSES,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    pretrained='vggface2',\n",
    "    s=30.0,\n",
    "    m=0.5\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer - Different LR for backbone and head\n",
    "optimizer = torch.optim.SGD([\n",
    "    {'params': model.backbone.parameters(), 'lr': 0.001},  # Lower LR for pretrained backbone\n",
    "    {'params': model.margin_product.parameters(), 'lr': 0.01} # Higher LR for new head\n",
    "], momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=[0.001, 0.01],\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.2,\n",
    "    div_factor=10.0,\n",
    "    final_div_factor=100.0\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training Loop\n",
    "best_val_acc = 0.0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model, train_loader, optimizer, criterion, scaler, scheduler, DEVICE, epoch\n",
    "    )\n",
    "    val_loss, val_acc = validate_one_epoch(\n",
    "        model, val_loader, criterion, DEVICE, epoch\n",
    "    )\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{EPOCHS}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"/kaggle/working/best_facenet_arcface.pth\")\n",
    "        print(f\"  ğŸ”¥ New best model saved! ({best_val_acc:.4f})\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_acc'], label='Train Acc')\n",
    "plt.plot(history['val_acc'], label='Val Acc')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8826230,
     "sourceId": 13855290,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venvDLproject (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
